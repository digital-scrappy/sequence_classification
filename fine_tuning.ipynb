{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c325a273-789f-4b99-a1cc-7fe55b61fdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from strip_headers import strip_headers\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import evaluate\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f78d946e-13f0-42b7-b012-eacc6e698882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GutenbergDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a57df15-f360-4a43-8d37-d43a10cb4273",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_people(people, text):\n",
    "    new_text = text\n",
    "    for person in people:\n",
    "        new_text = re.sub(person, 'Person', new_text, flags=re.IGNORECASE)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db8c799-416a-46b4-9993-b848bc72b9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting de-core-news-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.5.0/de_core_news_sm-3.5.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from de-core-news-sm==3.5.0) (3.5.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.10.5)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (67.3.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->de-core-news-sm==3.5.0) (2.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download de_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33608e06-69ee-41a0-b548-be817413a19a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "data_dir = Path.cwd() / \"data\"\n",
    "metadata = pl.read_csv(data_dir / \"metadata.csv\")\n",
    "\n",
    "\n",
    "\n",
    "authors = [\"Goethe, Johann Wolfgang von\", \"Schiller, Friedrich\"]\n",
    "author_mapping = {\"Goethe, Johann Wolfgang von\": 0,\n",
    "                  \"Schiller, Friedrich\": 1}\n",
    "metadata = metadata.filter(pl.col(\"language\") == \"['de']\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-german-cased\", num_labels=2)\n",
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40c8d012-1e0b-4168-b804-2ef28dfec509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"people.txt\", \"r\") as f:\n",
    "    #removing \\n \n",
    "    people = [person[:-1] for person in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf07ff41-0d72-4ed9-a8c4-9b89b898a404",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4750 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "#train test split, preprocessing, tokenization and splitting into blocks in no particular order\n",
    "\n",
    "train_encodings = {'input_ids'      : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                   'token_type_ids' : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                   'attention_mask' : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                   }\n",
    "train_labels = []\n",
    "\n",
    "test_encodings = {'input_ids'      : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                  'token_type_ids' : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                  'attention_mask' : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                  }\n",
    "test_labels = []\n",
    "\n",
    "val_encodings = {'input_ids'      : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                 'token_type_ids' : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                 'attention_mask' : torch.tensor([], requires_grad=False, dtype=torch.long),\n",
    "                 }\n",
    "val_labels = []\n",
    "\n",
    "\n",
    "train_encodings ={'input_ids'      : [],\n",
    "                  'token_type_ids' : [],\n",
    "                  'attention_mask' : [],\n",
    "                  }\n",
    "train_labels = []\n",
    "\n",
    "test_encodings ={'input_ids'      : [],\n",
    "                  'token_type_ids' : [],\n",
    "                  'attention_mask' : [],\n",
    "                  }\n",
    "test_labels = []\n",
    "\n",
    "val_encodings ={'input_ids'      : [],\n",
    "                  'token_type_ids' : [],\n",
    "                  'attention_mask' : [],\n",
    "                  }\n",
    "val_labels = []\n",
    "\n",
    "\n",
    "train_ids = []\n",
    "test_ids = []\n",
    "val_ids = []\n",
    "\n",
    "removed_chars = \"\\r\\n\\t.:~()[]{}\"\n",
    "\n",
    "\n",
    "for author in authors:\n",
    "    \n",
    "    #select texts by the authors from the whole gutenberg corpus\n",
    "    author_ids = metadata.filter((pl.col(\"author\") == author) & (pl.col(\"subjects\").str.contains(\"Drama\")))[\"id\"].to_list()\n",
    "    \n",
    "    #splitting in such a way that the model has never seen any parts of the play before\n",
    "    #otherwise it would probably just learn to recognize the names of the characters\n",
    "    #it will still ... \n",
    "    #I will just have very bad test loss\n",
    "    \n",
    "    #TODO: implement name removal\n",
    "    #https://stackoverflow.com/questions/53534376/removing-names-from-noun-chunks-in-spacy\n",
    "    \n",
    "    train_ids += author_ids[:-2]\n",
    "    test_ids += [author_ids[-2]]\n",
    "    val_ids += [author_ids[-1]] \n",
    "    \n",
    "    \n",
    "    for doc_id in author_ids:\n",
    "        \n",
    "        file_path = data_dir / \"raw\" / (doc_id + \"_raw.txt\")\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"r\") as in_f:\n",
    "                raw_text = in_f.read()    \n",
    "        except FileNotFoundError:\n",
    "            print(f\"Warning file not found{file_path}\")\n",
    "            continue\n",
    "            \n",
    "        \n",
    "        #script for removing the gutenberg project headers        \n",
    "        text = strip_headers(raw_text)         \n",
    "        \n",
    "        #removing all people from the people file\n",
    "        if doc_id in train_ids:\n",
    "            text = remove_people(people, text)\n",
    "           \n",
    "        \n",
    "        #more custom header and footer stripping\n",
    "        \n",
    "        #this book has a longer appendix\n",
    "        if doc_id =='PG47804':\n",
    "            \n",
    "            a = re.search(r'\\b(Fußnoten)\\b', text)\n",
    "            text = text[1000:-a.start()]\n",
    "            \n",
    "        # for the rest we just strip another 1000 chars\n",
    "        else: \n",
    "            text = text[1000:-1000]\n",
    "\n",
    "        text.strip(removed_chars) \n",
    "        #encoding the data first \n",
    "        #since I probably want to use the full 512 tokens without doing any truncating or padding\n",
    "        encoding = tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # might be inefficient but fine for now\n",
    "        # input ids here are token ids not doc ids ...\n",
    "\n",
    "        encodings = {'input_ids'      : torch.split(encoding['input_ids'], 32, dim=1)[:-1],\n",
    "                     'token_type_ids' : torch.split(encoding['token_type_ids'], 32, dim=1)[:-1],\n",
    "                     'attention_mask' : torch.split(encoding['attention_mask'], 32, dim=1)[:-1],\n",
    "                    }\n",
    "                     \n",
    "        \n",
    "        #not dry it's pretty damp\n",
    "        if doc_id in train_ids:\n",
    "            train_encodings = {key : list(encodings[key]) + train_encodings[key] for key in encodings}\n",
    "            train_labels += [author_mapping[author] for _ in range(len(encodings['input_ids']))]\n",
    "            # train_labels += [author_mapping[author].clone().detach() for _ in range(len(encoding['input_ids']))]\n",
    "        elif doc_id in test_ids:\n",
    "            test_encodings = {key : list(encodings[key]) + test_encodings[key] for key in encodings}\n",
    "            test_labels += [author_mapping[author] for _ in range(len(encodings['input_ids']))]\n",
    "            # test_labels += [author_mapping[author].clone().detach() for _ in range(len(encoding['input_ids']))]\n",
    "        elif doc_id in val_ids:\n",
    "            val_encodings = {key : list(encodings[key]) + val_encodings[key] for key in encodings}\n",
    "            val_labels += [author_mapping[author] for _ in range(len(encodings['input_ids']))]\n",
    "            # val_labels += [author_mapping[author].clone().detach() for _ in range(len(encoding['input_ids']))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3de8ce5-c03f-43ad-a628-15fce07622db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_encodings[\"input_ids\"]      = torch.split(torch.cat(train_encodings['input_ids'],dim=0),16, dim=0)\n",
    "# train_encodings[\"token_type_ids\"] = torch.split(torch.cat(train_encodings['token_type_ids'],dim=0),16, dim=0)\n",
    "# train_encodings[\"attention_mask\"] = torch.split(torch.cat(train_encodings['attention_mask'],dim=0),16, dim=0)\n",
    "# train_labels = torch.split(torch.cat(train_labels, dim=0), 16, dim=0)\n",
    "\n",
    "\n",
    "# test_encodings[\"input_ids\"]      = torch.split(torch.cat(test_encodings['input_ids'],dim=0),16, dim=0)\n",
    "# test_encodings[\"token_type_ids\"] = torch.split(torch.cat(test_encodings['token_type_ids'],dim=0),16, dim=0)\n",
    "# test_encodings[\"attention_mask\"] = torch.split(torch.cat(test_encodings['attention_mask'],dim=0),16, dim=0)\n",
    "# test_labels = torch.split(torch.cat(test_labels, dim=0), 16, dim=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# val_encodings[\"input_ids\"]      = torch.split(torch.cat(val_encodings['input_ids'],dim=0),16, dim=0)\n",
    "# val_encodings[\"token_type_ids\"] = torch.split(torch.cat(val_encodings['token_type_ids'],dim=0),16, dim=0)\n",
    "# val_encodings[\"attention_mask\"] = torch.split(torch.cat(val_encodings['attention_mask'],dim=0),16, dim=0)\n",
    "# val_labels = torch.split(torch.cat(val_labels, dim=0), 16, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5504722-b38d-4dd8-b30f-430630721882",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for split in [train_encodings, test_encodings, val_encodings]:\n",
    "    for key in split.keys():\n",
    "        split[key] = [ torch.squeeze(seq) for seq in split[key]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba314c87-3064-4309-bb8b-6769e6675c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = GutenbergDataset(train_encodings, train_labels)\n",
    "test_dataset = GutenbergDataset(test_encodings, test_labels)\n",
    "val_dataset = GutenbergDataset(val_encodings, val_labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "77a72462-975b-4209-8dc9-0bd568192096",
   "metadata": {
    "tags": []
   },
   "source": [
    "from transformers import  Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size= 64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b04254f-9382-4dcd-8abb-34ea5e40e7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "eval_loader = DataLoader(val_dataset, batch_size=8)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "597cb7a4-1636-4fc4-b55d-4245e8bd7334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ded84d141fc425dac9085b99ba51e05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_metric = evaluate.load(\"accuracy\")\n",
    "val_metric = evaluate.load(\"accuracy\")\n",
    "num_epochs = 7\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "optim = AdamW(model.parameters(), lr=4e-5)\n",
    "lr_scheduler = get_scheduler(name=\"linear\", optimizer=optim, num_warmup_steps=500, num_training_steps=num_training_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1abf041d-3ee3-49b5-8f48-7cc75667dbc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0601541a5f9d41ae93debfcb3efc95f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17885 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6981312268972397\n",
      "0.6749981753528118\n",
      "0.6621153472860655\n",
      "0.6389075046032667\n",
      "0.6183659367263317\n",
      "0.6025982542087635\n",
      "0.590913766260658\n",
      "0.5822002391517163\n",
      "0.5705316083878279\n",
      "0.5595698968544602\n",
      "0.5506713848493316\n",
      "0.5436614143103361\n",
      "0.5405218811218555\n",
      "0.5367326222147261\n",
      "0.5321390666812659\n",
      "0.52604173053056\n",
      "0.5204451663090902\n",
      "0.5173937512685856\n",
      "0.5127881113635866\n",
      "0.5085218100585044\n",
      "0.5034001012040036\n",
      "0.5016161739504473\n",
      "0.4985649181076366\n",
      "0.49526323518250137\n",
      "0.49104332123547795\n",
      "{'accuracy': 0.871875}\n",
      "{'accuracy': 0.5029585798816568}\n",
      "0.48273519953947847\n",
      "0.4764701568638186\n",
      "0.4716200244960403\n",
      "0.4659744054627449\n",
      "0.4613474572086539\n",
      "0.4557882199131791\n",
      "0.45381815048326635\n",
      "0.4509327900375277\n",
      "0.4472023490266457\n",
      "0.4432174834641517\n",
      "0.4399947926200794\n",
      "0.43831260121302223\n",
      "0.4343034257462439\n",
      "0.43099205215872466\n",
      "0.4280692298897759\n",
      "0.42535851676371195\n",
      "0.42183388470596556\n",
      "0.4189004816512666\n",
      "0.415895166051157\n",
      "0.41262191048200714\n",
      "0.409852102877038\n",
      "0.40834976532520995\n",
      "0.40669959392954896\n",
      "0.40510565887358485\n",
      "0.4031235411129413\n",
      "{'accuracy': 0.934375}\n",
      "{'accuracy': 0.5029585798816568}\n",
      "0.3980702890168997\n",
      "0.39350947658180585\n",
      "0.39026275718156006\n",
      "0.3876038637777077\n",
      "0.3849741432838202\n",
      "0.3824428766462691\n",
      "0.37970320077191416\n",
      "0.3767583795313372\n",
      "0.3736482754679003\n",
      "0.371181873564491\n",
      "0.36771823931942826\n",
      "0.3644492771902318\n",
      "0.3614449559195859\n",
      "0.3588746964098162\n",
      "0.35617233390353464\n",
      "0.3533585226191721\n",
      "0.3511186870718279\n",
      "0.3489928117445798\n",
      "0.3461672622527125\n",
      "0.34412199664486337\n",
      "0.3418116780664467\n",
      "0.33968056657169365\n",
      "0.33752927406673416\n",
      "0.33530254409259086\n",
      "0.33347972035230566\n",
      "{'accuracy': 0.98}\n",
      "{'accuracy': 0.5266272189349113}\n",
      "0.328971487105478\n",
      "0.32633330967988955\n",
      "0.3236261091105013\n",
      "0.3208178331988869\n",
      "0.3180193792937628\n",
      "0.31525979504703067\n",
      "0.3126584663706594\n",
      "0.3100560325650059\n",
      "0.3076976715469373\n",
      "0.30520583768557935\n",
      "0.30288988344261875\n",
      "0.3003109935322042\n",
      "0.2983049623771429\n",
      "0.2959715735034347\n",
      "0.2938518124705829\n",
      "0.2918705347683296\n",
      "0.29010257503263664\n",
      "0.28776372366712016\n",
      "0.286043978463632\n",
      "0.284117963334651\n",
      "0.28221939799567297\n",
      "0.280617628783496\n",
      "0.27863876614928745\n",
      "0.27716371026800385\n",
      "0.27537211767271613\n",
      "{'accuracy': 0.989375}\n",
      "{'accuracy': 0.5266272189349113}\n",
      "0.2721880002307359\n",
      "0.2701022390000573\n",
      "0.268130660871112\n",
      "0.2659302421089563\n",
      "0.2637393354825717\n",
      "0.26167663180172684\n",
      "0.25975576370354103\n",
      "0.25799072582564975\n",
      "0.25615895252173143\n",
      "0.2541153852663496\n",
      "0.25228104730582096\n",
      "0.25030279347658074\n",
      "0.24847244442953526\n",
      "0.24665380742941084\n",
      "0.24509452886708794\n",
      "0.24318840500585265\n",
      "0.24150616151518806\n",
      "0.23986095859536075\n",
      "0.2382491213808673\n",
      "0.23666099183348657\n",
      "0.23500129714778847\n",
      "0.23336572927347274\n",
      "0.23188630582567807\n",
      "0.23036957363218546\n",
      "0.22882329510681737\n",
      "{'accuracy': 0.99625}\n",
      "{'accuracy': 0.5029585798816568}\n",
      "0.22649537183688864\n",
      "0.22484538873391557\n",
      "0.2233312099167621\n",
      "0.221881042562206\n",
      "0.22033499650482663\n",
      "0.21882852255403298\n",
      "0.2173228301637087\n",
      "0.21593817944454483\n",
      "0.21456406838587064\n",
      "0.21318084939565515\n",
      "0.2117584414133374\n",
      "0.21041192890508084\n",
      "0.2089712694379109\n",
      "0.20768979644882074\n",
      "0.20632032525525862\n",
      "0.20507251680153218\n",
      "0.20378896616423817\n",
      "0.2024655743404595\n",
      "0.2013432973273435\n",
      "0.20003935817687798\n",
      "0.1987301110225268\n",
      "0.19748162294137844\n",
      "0.19627616666538863\n",
      "0.19511416835836604\n",
      "0.19393404817215507\n",
      "{'accuracy': 0.99875}\n",
      "{'accuracy': 0.5266272189349113}\n",
      "0.19229321781514114\n",
      "0.1911356211798522\n",
      "0.1899837700093008\n",
      "0.18880454818104606\n",
      "0.1877173439734411\n",
      "0.18655744948403133\n",
      "0.18548144527389035\n",
      "0.18434777143571202\n",
      "0.18327405110186767\n",
      "0.18221803421143354\n",
      "0.1811235481049036\n",
      "0.18020411469881786\n",
      "0.17920703907102828\n",
      "0.17814605923778545\n",
      "0.17716007835513387\n",
      "0.17614579550060044\n",
      "0.17520756636082394\n",
      "0.17420600155864283\n",
      "0.17331001672955787\n",
      "0.17233360626041744\n",
      "0.17145553506443004\n",
      "0.1705934971639835\n",
      "0.16973457197333244\n",
      "0.1688053736210529\n",
      "0.16786559021378675\n",
      "{'accuracy': 0.99875}\n",
      "{'accuracy': 0.5207100591715976}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.to(device)\n",
    "losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    counter = 0\n",
    "    for batch in train_loader:\n",
    "        counter += 1\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        losses.append(loss.item())\n",
    "        if counter % 100 == 0:\n",
    "            print(sum(losses) / len(losses))\n",
    "        lr_scheduler.step()\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "    counter = 0\n",
    "    for batch in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        \n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "\n",
    "        train_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "        \n",
    "        if counter == 200:\n",
    "            break\n",
    "    print(train_metric.compute())\n",
    "    \n",
    "    for batch in eval_loader:\n",
    "\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        \n",
    "\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "\n",
    "        val_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "    print(val_metric.compute())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4147753-b9d4-46bc-825c-2a31986ef808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1a69e-0955-4328-a5f6-fabbd1326a53",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
